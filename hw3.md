---
img: pianopegs
img_link: https://www.flickr.com/photos/fdmount/3481925964/
caption: Image by fdmount
title: Reranking | Homework 3
active_tab: hw3 
---
<div class="alert alert-danger">
  This page contains the 2016 version of this assignment.
  The assignments for 2017 are likely to be substantially different.
</div>

Reranking <span class="text-muted">| Homework 3</span>
==============================================================

In [homework 2](hw2.html) you learned to search for probable translations,
but saw that this is only useful with a good statistical model. In class
you learned about measures of translation accuracy that correlate, at least
somewhat, with human perception of translation quality. Armed with one such 
metric, BLEU, you now have an objective way to measure a model's usefulness. In 
this assignment we will give you some sentences from Russian news articles, 
and you will use the metric to guide changes to a model that chooses from a set of
possible English translations generated by a state-of-the-art machine
translation system. __Your challenge is to choose the best 
translations according to BLEU.__ 

Getting Started
---------------

Get the latest changes from the homework repo.

    git pull origin master
  
Or, get a fresh copy.

    git clone https://github.com/alopez/infr11062.git

Under the `reranker` directory, you will find several programs and a data
directory containing a dataset that has been partitioned into three parts:

  1. Training data, from which you can learn a model.
  1. Development data, on which you can experiment with learned models
     to see how they change the resulting translations.
  1. Test data, on which you will aim to improve translation.
    
Each partition of the data consists of input sentences, a list of
possible translations and their associated features according to a baseline
model, and, for all but the test data, a set of reference translations.

Let's examine the development data. First, look at the first
five input sentences.

    head -5 data/dev+test.src

Unless you read Russian, this might not make sense to you. Let's look
at the corresponding English translations, produced by a professional
translators.

    head -5 data/dev.ref 

Using a [state-of-the-art translation system](http://www.cdec-decoder.org/)
we have obtained the 100-best translations according to its model. 
Let's look at the translations for sentence 100401, the first one in
the source file.

    grep 100401 data/dev+test.100best

On each line, you will see the sentence number, the translation itself,
and the values of three features that the decoder computed for that
translation. Each feature is a function from the input and output
sentence pair to a real value.
We can use these features to choose the best 
translation candidate according to a linear model.

    mkdir workdir
    python2.7 rerank > workdir/default.out 
    head -5 workdir/default.out

Every candidate translation $$e\in E(f)$$ of
an input sentence $$f$$ has an associated feature vector 
$$h(e,f) = [ \log p_{LM}(e)$$, $$\log p_{TM}(e|f)$$, $$\log p_{TM_{Lex}}(f|e)]$$. The
reranker takes a parameter vector $$\theta$$ whose length is equal to
that of $$h(e,f)$$. By default, $$\theta = [1, 1, 1]$$. For
each $$f$$, the reranker returns $$\hat{e}$$ according to the
following decision function.

<center>
$$\hat{e} = \arg\max_{e\in E(f)} \theta \cdot h(e,f)$$
</center>

To evaluate translations on the development set, compute BLEU
score against their reference translations.

    python2.7 compute-bleu < workdir/default.out

BLEU is computed as:

<center>
$$\textrm{BLEU} = \textrm{BP} \cdot \exp\left(\sum_{n=1}^4 \frac{1}{4} \log p_n \right)$$
</center>

where $$p_n$$ is $$n$$-gram precision of the translations, and the 
BLEU penalty BP is computed from the reference length $$r$$ and 
candidate translation length $$c$$ as: 

<center>
$$\textrm{BP} = \left\{ \begin{array}{ll}1 & \textrm{if }c > r \\ \textrm{exp}(1-\frac{r}{c}) & \textrm{otherwise}\end{array}\right.$$
</center>

The script `compute-bleu` reports  100 $$times$$ BLEU, giving  a number
between 0 and 100. This makes it look much like a percentage, though it
shouldn't be interpreted as one.

You can change the weight assigned to each feature at the 
command line.
What happens if you change the sign of the language model weight?

    python2.7 rerank -w 'p(e)=-1' > workdir/fliplm.out

**Question 1 [5 marks]. What is the mathematical interpretation of changing
the sign on the language model feature? What would you expect
to happen? How do you think it would it change the translations?**

**Question 2 [10 marks]. Now compare the translations produced by the default reranker, and
the ones you get by changing the sign of the language model. What do you notice?
Does one set of translations seem better than the other? In what way? Does
it confirm your hypothesis about what should happen?
Support your answers with evidence from the data.**

Now that you've assessed the output qualitatively, let's assess it
quantitatively.

    python2.7 compute-bleu < workdir/fliplm.out

Ok, we've seen what happens if we change sign of the language model feature.
What happens if we use only this feature? Try zeroing out the other features
and observe the effect on BLEU.

    python2.7 rerank -w 'p(e|f)=0 p_lex(f|e)=0' | python2.7 compute-bleu

**Question 3 [10 marks]. Compare the BLEU score of translations produced by zeroing out
each feature in turn. Compare the BLEU score of translations produced by 
zeroing out all but one feature, for each feature in turn. Compare the
BLEU scores resulting from flipping the sign of each feature. What do you observe?
What do you conclude about the relative importance of each feature according to BLEU?**

Now you have a way to change the accuracy of the resulting translations, at least
according to BLEU, by modifying the vector $$\theta$$.
If you could set the weights perfectly, what's the best BLEU score you
could achieve? To give you an idea, we've given you an omniscient *oracle* for the
development data. Using knowledge of the reference translation, it chooses
candidate sentences that maximize the BLEU score.

    python2.7 oracle | tee workdir/oracle.out | python2.7 compute-bleu

The oracle should convince you that it is possible to do _much_
better than the default reranker in terms of BLEU score, and perhaps even in
terms of actual translation quality. 

**Question 4 [10 marks]. Compare the translations produced by the default reranker and 
the oracle. Does one set of translations seem better than the other? Why or
why not? Support your answer with evidence from the data.**

Baseline
--------

Now that we've seen how $$\theta$$ affects the resulting translations and
their BLEU scores, let's attempt to improve BLEU more systematically.

**Question 5 [20 marks]. Try many different settings of the three feature
weights. You can do this by automatically generating many different feature
weights.[^1] How much can you improve
BLEU score over the default decoder? What do you notice about the resulting
translations? Support your answer with evidence from the data.**

[^1]: Doing this systematically is sometimes called _grid search_.

Varying the $$\theta$$ systematically is one way to improve BLEU. A second
is to add new features to the data, which increases the length of $$\theta$$,
and gives us more parameters to tune.

Let's develop a new feature. To do this we'll need to think about properties
of the data and how they interact with our metric. A simple property of 
the data that the oracle optimizes is the number of
words in the output, which affects the BLEU penalty BP. 

    wc -w < data/dev.ref
    head -400 workdir/oracle.out | wc -w 
    head -400 workdir/default.out | wc -w

Notice that the oracle produces output of length quite close to that of 
the reference that it had access to.
The number of output words wouldn't seem to matter much for the
overall readability of the output. But it is useful to encourage the 
translation system to produce translations of a reasonable length. If we
don't, the system could learn to maximize its precision by
guessing only very certain words and dropping the rest, a situation we
wish to avoid. We can see this effect in the above results, where the
translations of the default decoder are around 10% shorter than those
in the reference. One way to interpret this is that
the default decoder drops one in ten words, which seems suspicious.

If we want our system to be sensitive to a property of real translations,
we must model that property. Since our model is just a linear combination
of features, our observation about length is easy to model. We simply make
the output length a feature of the model. Once we have this feature, we
can control the output length by varying the corresponding feature weight.

    cat data/dev+test.100best | cut -f 4 -d \| | awk '{print "len="NF}' > workdir/dev+test.len.txt
    paste -d \  data/dev+test.100best workdir/dev+test.len.txt > workdir/dev+test.with-len.txt

The new file `data/dev+test.with-len.txt` contains the same translations
with a new length feature. Our default reranker will pick up this 
feature and assign it weight of 1. Try it:

    python2.7 rerank -k workdir/dev+test.with-len.txt > workdir/withlen.txt
    python2.7 compute-bleu < workdir/withlen.txt

Convince yourself that this change gives you a parameter to control the
length of the output.

**Question 6 [15 marks]. Experiment with several values for the length parameter,
holding the other parameters constant. What do you observe? How does
the parameter affect the resulting output length and BLEU scores?
Why is BLEU affected in this way?**

The Challenge
-------------

Answering the questions above should give you some intuitions about how
linear models work, and how feature engineering can be used to affect the
output translations. However, you should be convinced that you can improve BLEU 
by adding more features, changing the parameter values, or combining
both strategies. As you add more features, finding good weights by
trial and error will become less and less efficient. To really make 
progress you need automation in the form of effective learning algorithms 
that optimize $$\theta$$ for BLEU. Your task is to improve translation 
quality **on the blind test set** as much as possible by adding features, 
improving the learning component, or some combination of these strategies.
I strongly advise you to train your learning algorithm on the training
data, and test it on the development data. If you learn and test your
algorithm on the same data, you may get an unrealistically positive view
of how it will perform on the test set, for which you do not have 
references to compute BLEU score.

**Question 7 [30 marks]. Explain the extension you make to the 
reranking system. Motivate any features, describe any learning algorithms,
and also describe experiments that you ran on the development data to
test the accuracy of your new reranking system.**

A reasonable candidate for your learning algorithm is 
PRO (pairwise ranking optimization), described in the
following paper:

> [Tuning as Ranking](http://www.aclweb.org/anthology/D11-1125). Mark Hopkins and Jonathan May. EMNLP 2011.

One difference from the paper is that you will not be able to run a
decoder at each iteration. Instead, you can iterate over the
n-best lists provided to you. This approach is called *batch
tuning* and explained further in:

> [Batch Tuning Strategies for Statistical Machine Translation](http://aclweb.org/anthology-new/N/N12/N12-1047v2.pdf). Colin Cherry and George Foster. In NAACL 2012.

Here is a pseudocode version of PRO:

    Parameters:
        tau: samples generated from n-best list per input sentence (set to 5000)
        alpha: sampler acceptance cutoff (set to 0.1)
        xi: training data generated from the samples tau (set to 100)
        eta: perceptron learning rate (set to 0.1)
        epochs: number of epochs for perceptron training (set to 5)

    for each sentence i:
        collect all the n-best outputs for i
        for each candidate c in the n-best list:
            compute the bleu score b (using bleu.py) for c
            append (c,b) to nbests[i]

    for i = 1 to epochs:
        for nbest in nbests:
            get_sample():
                initialize sample to empty list 
                loop tau times:
                    randomly choose two items from nbest list, s1 and s2
                    if fabs(s1.smoothed_bleu - s2.smoothed_bleu) > alpha:
                        if s1.smoothed_bleu > s2.smoothed_bleu:
                            sample += (s1, s2)
                        else:
                            sample += (s2, s1)
                    else:
                        continue
                return sample
            sort the tau samples from get_sample() using s1.smoothed_bleu - s2.smoothed_bleu
            keep the top xi (s1, s2) values from the sorted list of samples
            do a perceptron update of the parameters theta:
               if theta * s1.features <= theta * s2.features:
                   mistakes += 1
                   theta += eta * (s1.features - s2.features) # this is vector addition!
    return theta

Implementing a batch tuning version of PRO would be a reasonable extension. 
However, there are many other methods that might improve BLEU. Here are some ideas:

  * Improve the learning algorithm in PRO (from the perceptron to averaged perceptron, for instance).
  * Implement the [minimum error rate training (MERT) algorithm](http://www.aclweb.org/anthology/P03-1021) used by Google Translate.
  * Add features to `train.nbest` and `test.nbest`
      * Add a feature to count words that appear to be untranslated.
      * Add [an IBM Model 1 score](http://aclweb.org/anthology/N/N04/N04-1021.pdf) (sum over all alignments) as a feature.
  * Use [ordinal regression or uneven margins](http://aclweb.org/anthology/N/N04/N04-1023.pdf).
  * Many, many ideas to improve reranking: using [hope and fear](http://www3.nd.edu/~dchiang/papers/chiang-jmlr12-corrected.pdf).

You can even change the decision rule that you use to return translations. 
For instance, you might try using the 
[minimum Bayes risk](http://aclweb.org/anthology//N/N04/N04-1022.pdf)
decision rule, which chooses translations to maximize expected BLEU
with respect to all other translations, rather than the highest-scoring
translation.

But the sky's the limit! You can try anything you want, as long as
you follow the ground rules.

Ground Rules
------------

* You may work individually or in pairs. I __encourage__ you to work in 
  pairs, and I strongly recommend that you share and communicate about all 
  aspects of the work, since this facilitates learning. __You may not
  work with the same partner that you worked with on either of the previous
  assignments__. I want
  you to collaborate with different people, ideally with different skills
  from yours. No more than two people may work together, and different 
  groups may not share code or answers. Your code and report must be your 
  own work. But sharing questions, clarifications and ideas, 
  especially through [the forum](https://piazza.com/class/idfwi88bkpo377) is great! If you 
  work with a partner, you will both receive the same mark. I will not 
  adjudicate [Rashomon](https://en.wikipedia.org/wiki/Rashomon)-style 
  stories about who did or did not contribute.

* You must submit six files. Your names __must not appear__ in any of them:
    1. `answers.pdf`: A file containing your answers to Questions 1 through 7 in an A4 PDF. The text portion of your answer must not exceed two pages, so be concise. Apart from question 9, most questions can be answered with a few sentences and a picture, and I will not require the markers to read more than two pages of text. Figures and tables should appear at the end of the text and may take as many pages as required. They should be numbered and the text should refer to these numbers.
    1. `myreranker.out`: Your translations of the complete dataset using your modified reranker.
    1. Any implementation files associated with your modification to the reranker. You are not required to use the provided python code, and may roll your own solution in whatever language you prefer. If you do, please name your implementation files according to the conventions of your language. If your implementation requires more than one source file, please submit all of them.
    1. `authors.txt`: A text file containing the student IDs of the person or persons who worked on the assignment, one per line.

On dice, run:

    submit mt 3 answers.pdf myreranker.out authors.txt

### Acknowledgements

This assignment was developed in collaboration with
[Chris Callison-Burch](http://www.cis.upenn.edu/~ccb/),
[Chris Dyer](http://www.cs.cmu.edu/~cdyer),
[Matt Post](http://cs.jhu.edu/~post/), and
[Anoop Sarkar](http://www.cs.sfu.ca/~anoop/).
