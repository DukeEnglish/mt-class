---
img: pianopegs
img_link: https://www.flickr.com/photos/fdmount/3481925964/
caption: Image by fdmount
title: Reranking | Homework 3
active_tab: hw3 
---

<div class="alert alert-info">
  This assignment is due 12 March at 16:00.
</div>

Reranking <span class="text-muted">| Homework 3</span>
==============================================================

In [homework 2](hw2.html) you learned to search for probable translations,
but saw that this is only useful with a good probability model. In class
you learned about measures of translation accuracy that correlate, at least
somewhat, with human perception of translation quality. Armed with one such 
metric, BLEU, you now have an objective way to measure a model's usefulness. In 
this assignment we will give you some sentences from Russian news articles, 
and you will use the metric to improve a model that chooses from a set of
possible English translations generated by a state-of-the-art machine
translation system. __Your challenge is to choose the best 
translations according to BLEU.__ 

<div class="alert alert-info">
  Note: This assignment requires python 2.7. On dice, you may need
  to substitute <code>python2.7</code> for <code>python</code> in 
  the commands listed below.
</div>

Getting Started
---------------

Get the latest changes from the homework repo.

    git pull origin master
  
Or, get a fresh copy.

    git clone https://github.com/alopez/infr11062.git

Under the `reranker` directory, you will find several programs and a data
directory containing a dataset that has been partitioned into three parts:

  1. Training data, from which you can learn a model.
  1. Development data, on which you can experiment with learned models
     to see how they change the resulting translations.
  1. Test data, on which you will aim to improve translation.
    
Each partition of the data consists of input sentences, a list of
possible translations and their associated features according to a baseline
model, and, for all but the test data, a set of reference translations.

Let's examine the development data. First, look at the first
five input sentences sentences.

    head -5 data/dev+test.src

Unless you read Russian, this might not make sense to you. Let's look
at the corresponding English translations, produced by a professional
translators.

    head -5 data/dev.ref 

Using a [state-of-the-art translation system](http://cdec-decoder.org/)
we have obtained the 100-best translations according to its model. 
Let's look at the translations for sentence 100401, the first one in
the source file.

    grep 100401 data/dev+test.100best

On each line, you will see the sentence number, the translation itself,
and the values of three features that the decoder computed for that
translation. Each feature is a function from the input and output
sentences to a real value.
We can use these features to choose the best 
translation candidate according to a linear model.

    mkdir workdir
    python rerank > workdir/default.out

Look at the output.

    head -5 workdir/default.out

Every candidate translation $$e\in E(f)$$ of
an input sentence $$f$$ has an associated feature vector 
$$h(e,f) = [ \log p_{LM}(e)$$, $$\log p_{TM}(f|e)$$, $$\log p_{TM_{Lex}}(f|e)]$$. The
reranker takes a parameter vector $$\theta$$ whose length is equal to
that of $$h(e,f)$$. By default, $$\theta = [1, 1, 1]$$. For
each $$f$$, the reranker returns $$\hat{e}$$ according to the
following decision function.

<center>
$$\hat{e} = \arg\max_{e\in E(f)} \theta \cdot h(e,f)$$
</center>

To evaluate translations on the development set, compute BLEU
score against their reference translations.

    python compute-bleu < workdir/default.out

BLEU is computed as:

<center>
$$\textrm{BLEU} = \textrm{BP} \cdot \exp\left(\sum_{n=1}^4 \frac{1}{4} \log p_n \right)$$
</center>

where $$p_n$$ is $$n$$-gram precision of the translations, and the 
BLEU penalty BP is computed from the reference length $$r$$ and 
candidate translation length $$c$$ as: 

<center>
$$\textrm{BP} = \left\{ \begin{array}{ll}1 & \textrm{if }c > r \\ \textrm{exp}(1-\frac{r}{c}) & \textrm{otherwise}\end{array}\right.$$
</center>

The script `compute-bleu` reports  100 $$times$$ BLEU, which is a number
between 0 and 100, much like a percentage.

You can change the weight assigned to each feature at the 
command line.
What happens if you change the sign of the language model weight?

    python rerank -w 'p(e)=-1' > workdir/fliplm.out
    python compute-bleu < workdir/fliplm.out

**Question 1. Compare the translations produced by the default reranker, and
the ones you get by changing the sign of the language model. What do you notice?
Does one set of translations seem better than the other? In what way?**

Changing the sign of a feature's weight simply causes the reranker to invert the
feature's ranking of candidate translations. Since changing the sign of the
language model feature made things worse, it is clear that the language 
model is important to translation accuracy.
What happens if we use only this feature? Try zeroing out the other features
and observe the effect on BLEU.

    python rerank -w 'p(e|f)=0 p_lex(f|e)=0' | python compute-bleu

**Question 2. Compare the BLEU score of translations produced by zeroing out
each feature in turn. Compare the BLEU score of translations produced by 
zeroing out all but one feature, for each feature in turn. Compare the
BLEU scores resulting from flipping the sign of each feature. What do you observe?
Can you conclude anything about the relative importance of each feature?**

Now you have a way to change the accuracy of the resulting translations, at least
according to BLEU, by modifying the vector $$\theta$$.
If you could set the weights perfectly, what's the best BLEU score you
could achieve? To give you an idea, we've given you an omniscient *oracle* for the
devlopment data. Using knowledge of the reference translation, it chooses
candidate sentences that maximize the BLEU score.

    python oracle > workdir/oracle.out
    python compute-bleu < workdir/oracle.out

The oracle should convince you that it is possible to do _much_
better than the default reranker in terms of BLEU score, and perhaps even in
terms of actual translation quality. Maybe you can improve the reranker by changing
the parameter vector $$\theta$$. 

**Question 3. Compare the translations produced by the default reranker and 
the oracle. Does one set of translations seem better than the other? How?**

**Question 4. Try setting the weights of the three features by hand. Experiment
with several different combinations of feature values. How much can you improve
BLEU score over the default decoder? What do you notice about the resulting
translations?**

Baseline
--------

A simple property of the data that the oracle optimizes is the number of
words in the output, which affects the BLEU penalty BP. 

    wc -w < data/dev.ref
    head -400 workdir/oracle.out | wc -w 
    head -400 workdir/default.out | wc -w

Notice that the oracle produces output of length quite close to that of 
the reference that it had access to.
The number of output words wouldn't seem to matter much for the
overall readability of the output. But it is useful to encourage the 
translation system to produce translations of a reasonable length. If we
don't, the system could learn to maximize its precision by
guessing only very certain words and dropping the rest, a situation we
wish to avoid. We can see this effect in the above results, where the
translations of the default decoder are around 10% shorter than those
in the reference. This means that the decoder drops one in ten words.

If we want our system to be sensitive to a property of real translations,
we must model that property. Since our model is just a linear combination
of features, our observation about length is easy to model. We simply make
the output length a feature of the model. By changing its associated 
parameter, we change the output length.

    cat data/dev+test.100best | cut -f 4 -d \| | awk '{print "len="NF}' > workdir/dev+test.len.txt
    paste -d \  data/dev+test.100best workdir/dev+test.len.txt > workdir/dev+test.with-len.txt

The new file `data/dev+test.with-len.txt` contains the same translations
with a new length feature. Our default reranker will pick up this 
feature and assign it weight of 1. Try it:

    python rerank -k workdir/dev+test.with-len.txt > workdir/withlen.txt
    python compute-bleu < workdir/withlen.txt

Convince yourself that this change gives you a parameter to control the
length of the output.

**Question 5. Experiment with several values for the length parameter,
holding the other parameters constant. What do you observe? How does
the parameter affect the resulting output length and BLEU scores?
Why is BLEU affected in this way?**

The Challenge
-------------

Answering the questions above will be enough to earn five points on 
the assignment. However, you should be convinced that you can improve BLEU 
by adding more features, changing the parameter values, or combining
both strategies. As you add more features, finding good weights by
trial and error will become less and less efficient. To really make 
progress you need automation in the form of effective learning algorithms 
that optimize $$\theta$$ for BLEU. Your task is to improve translation 
quality **on the blind test set** as much as possible by adding features, 
improving the learning component, or some combination of these strategies.
I strongly advise you to train your learning algorithm on the training
data, and test it on the development data. If you learn and test your
algorithm on the same data, you may get an unrealistically positive view
of how it will perform on the test set, for which you do not have 
references to compute BLEU score.

A reasonable candidate for your learning algorithm is 
PRO (pairwise ranking optimization), described in the
following paper:

> [Tuning as Ranking](http://www.aclweb.org/anthology/D11-1125). Mark Hopkins and Jonathan May. EMNLP 2011.

One difference from the paper is that you will not be able to run a
decoder at each iteration. Instead, you can iterate over the
n-best lists provided to you. This approach is called *batch
tuning* and explained further in:

> [Batch Tuning Strategies for Statistical Machine Translation](http://aclweb.org/anthology-new/N/N12/N12-1047v2.pdf). Colin Cherry and George Foster. In NAACL 2012.

Here is a pseudocode version of PRO:

    Parameters:
        tau: samples generated from n-best list per input sentence (set to 5000)
        alpha: sampler acceptance cutoff (set to 0.1)
        xi: training data generated from the samples tau (set to 100)
        eta: perceptron learning rate (set to 0.1)
        epochs: number of epochs for perceptron training (set to 5)

    for each sentence i:
        collect all the n-best outputs for i
        for each candidate c in the n-best list:
            compute the bleu score b (using bleu.py) for c
            append (c,b) to nbests[i]

    for i = 1 to epochs:
        for nbest in nbests:
            get_sample():
                initialize sample to empty list 
                loop tau times:
                    randomly choose two items from nbest list, s1 and s2
                    if fabs(s1.smoothed_bleu - s2.smoothed_bleu) > alpha:
                        if s1.smoothed_bleu > s2.smoothed_bleu:
                            sample += (s1, s2)
                        else:
                            sample += (s2, s1)
                    else:
                        continue
                return sample
            sort the tau samples from get_sample() using s1.smoothed_bleu - s2.smoothed_bleu
            keep the top xi (s1, s2) values from the sorted list of samples
            do a perceptron update of the parameters theta:
               if theta * s1.features <= theta * s2.features:
                   mistakes += 1
                   theta += eta * (s1.features - s2.features) # this is vector addition!
    return theta

Implementing a batch tuning version of PRO would be a reasonable extension. 
However, there are many other methods that might improve BLEU. Here are some ideas:

  * Improve the learning algorithm in PRO (from the perceptron to averaged perceptron, for instance).
  * Implement the [minimum error rate training (MERT) algorithm](http://www.aclweb.org/anthology/P03-1021) used by Google Translate.
  * Add features to `train.nbest` and `test.nbest`
      * Add a feature to count words that appear to be untranslated.
      * Add [an IBM Model 1 score](http://aclweb.org/anthology/N/N04/N04-1021.pdf) (sum over all alignments) as a feature.
  * Use [ordinal regression or uneven margins](http://aclweb.org/anthology/N/N04/N04-1023.pdf).
  * Many, many ideas to improve reranking: using [hope and fear](http://www3.nd.edu/~dchiang/papers/chiang-jmlr12-corrected.pdf).

You can even change the decision rule that you use to return translations. 
For instance, you might try using the 
[minimum Bayes risk](http://aclweb.org/anthology//N/N04/N04-1022.pdf)
decision rule, which chooses translations to maximize expected BLEU
with respect to all other translations, rather than the highest-scoring
translation.

But the sky's the limit! You can try anything you want, as long as
you follow the ground rules.

Ground Rules
------------

* You may work in independently or pairs, under these 
  conditions: 
    1. You must let me know about the collaboration in advance
       by emailing me and copying your collaborator on the email.
    1. You agree that everyone in the group will receive the same grade on the assignment. 
    1. You cannot undo a collaboration once you've informed me.
       I encourage collaboration since explaining things to someone else
       often helps you understand them better yourself. But I will not adjudicate Rashomon-style 
       stories about who did or did not contribute.
* You must turn in four things using the `submit mt 3 <files>` command on dice:
    1. A file containing your answers to Questions 1 through 5.
    1. A clear, mathematical description of your features and/ or 
       algorithm and your motivation
       written in scientific style. This needn't be long, but it should be
       clear enough that one of your fellow students could re-implement it.
    1. Your translations of `dev+test.src`, chosen from `dev+test.100best.txt`.
    1. Your code. 
       You are free to extend the code provided or roll your own in whatever
       langugage you like, but the code should be self-contained, 
       self-documenting, and easy to use. 

Your written work may be submitted in PDF, text, or markdown (like 
[this website](https://github.com/alopez/mt-class)). I will not accept
any other format. If you write the file in a different format, please
convert it to one of the above prior to submission.

### Acknowledgements

This assignment was developed in collaboration with
[Chris Callison-Burch](http://www.cis.upenn.edu/~ccb/),
[Chris Dyer](http://www.cs.cmu.edu/~cdyer),
[Matt Post](http://cs.jhu.edu/~post/), and
[Anoop Sarkar](http://www.cs.sfu.ca/~anoop/).
